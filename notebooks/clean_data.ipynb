{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Analytics for 311 Service Request Resolution\n",
    "---\n",
    "Methodology:\n",
    "- Data from over 45 cities was revied from Andew Friedman's 311 Dataset repository (**https://andrew-friedman.github.io/jkan/datasets/**).\n",
    "- We decided to include the target cites in our study as they showed avalible data which contained a start data, end date, and department from 2014/1/1 through 2023/12/31.\n",
    "- Target Cities: Baltimore, MD, Boston, MA, Buffalo, NY, Miami, FL, Oakland, CA, and Washington, DC\n",
    "- Target Date Range: 2014/1/1 through 2023/12/31 (10 Years)\n",
    "- Raw data from the target cities was downloaded and pre-processed using the clean_data function. While processing the function limits the file size to 1000mb as that is the GitHub file Size limit with our current plan.\n",
    "\n",
    "Functions:\n",
    "- split_csv(): Splits a CSV file into smaller chunks if it exceeds a given size in MB.\n",
    "- clean_city_data(): Clean city data from CSV files, filtering by date range, and sorting by open date. If the file is larger than 100 MB, it is split into smaller chunks before processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open_date</th>\n",
       "      <th>close_date</th>\n",
       "      <th>department</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2019-01-01 06:23:06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Enforcement Section</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2019-01-01 08:02:18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Enforcement Section</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2019-01-01 14:31:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Enforcement Section</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>2019-01-01 15:47:28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Enforcement Section</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>2019-01-01 16:07:25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Enforcement Section</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343664</th>\n",
       "      <td>2023-12-31 22:50:21</td>\n",
       "      <td>2024-01-19 14:21:41</td>\n",
       "      <td>Solid Waste Management</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343665</th>\n",
       "      <td>2023-12-31 23:02:40</td>\n",
       "      <td>2024-01-10 14:18:19</td>\n",
       "      <td>Solid Waste Management</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343666</th>\n",
       "      <td>2023-12-31 23:27:03</td>\n",
       "      <td>2024-01-30 21:18:17</td>\n",
       "      <td>Solid Waste Management</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343667</th>\n",
       "      <td>2023-12-31 23:37:38</td>\n",
       "      <td>2024-01-19 18:29:21</td>\n",
       "      <td>Solid Waste Management</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343668</th>\n",
       "      <td>2023-12-31 23:56:19</td>\n",
       "      <td>2024-01-30 15:51:54</td>\n",
       "      <td>Solid Waste Management</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1320633 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  open_date           close_date              department city\n",
       "55      2019-01-01 06:23:06                  NaN     Enforcement Section    m\n",
       "65      2019-01-01 08:02:18                  NaN     Enforcement Section    m\n",
       "133     2019-01-01 14:31:24                  NaN     Enforcement Section    m\n",
       "211     2019-01-01 15:47:28                  NaN     Enforcement Section    m\n",
       "280     2019-01-01 16:07:25                  NaN     Enforcement Section    m\n",
       "...                     ...                  ...                     ...  ...\n",
       "343664  2023-12-31 22:50:21  2024-01-19 14:21:41  Solid Waste Management    i\n",
       "343665  2023-12-31 23:02:40  2024-01-10 14:18:19  Solid Waste Management    i\n",
       "343666  2023-12-31 23:27:03  2024-01-30 21:18:17  Solid Waste Management    i\n",
       "343667  2023-12-31 23:37:38  2024-01-19 18:29:21  Solid Waste Management    i\n",
       "343668  2023-12-31 23:56:19  2024-01-30 15:51:54  Solid Waste Management    i\n",
       "\n",
       "[1320633 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Split CSV Function\n",
    "def split_csv(input_path, max_size_mb=1000):\n",
    "    \"\"\"\n",
    "    Splits a CSV file into smaller chunks if it exceeds a given size in MB.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to the input CSV file.\n",
    "        max_size_mb (int): Maximum allowed size in MB. Default is 1000 MB.\n",
    "\n",
    "    Returns:\n",
    "        List of paths to the split files.\n",
    "    \"\"\"\n",
    "    # Query File Size & Convert to Megabites\n",
    "    file_size_mb = os.path.getsize(input_path) / (1024 * 1024)\n",
    "\n",
    "    # Set File Size Limit from Function Call\n",
    "    if file_size_mb <= max_size_mb:\n",
    "        return [input_path]\n",
    "\n",
    "    # If file is larger than 100 MB, split it into smaller chunks\n",
    "    df = pd.read_csv(input_path)\n",
    "    chunk_size = int(np.ceil(len(df) * (max_size_mb / file_size_mb)))  # Calculate chunk size based on file size\n",
    "    split_files = []\n",
    "    base_name = os.path.splitext(input_path)[0]\n",
    "\n",
    "    for i, chunk in enumerate(np.array_split(df, len(df) // chunk_size + 1)):\n",
    "        split_file_path = f\"{base_name}_part{i+1}.csv\"\n",
    "        chunk.to_csv(split_file_path, index=False)\n",
    "        split_files.append(split_file_path)\n",
    "        print(f\"File '{input_path}' split into '{split_file_path}'\")  # Print message when file is split\n",
    "\n",
    "    return split_files\n",
    "\n",
    "# Clean City Data Function\n",
    "def clean_city_data(input_paths, output_path, city_names, date_open_col, date_closed_col, department_col):\n",
    "    \"\"\"\n",
    "    Clean city data from CSV files, filtering by date range, and sorting by open date.\n",
    "    If the file is larger than 100 MB, it is split into smaller chunks before processing.\n",
    "\n",
    "    Args:\n",
    "        input_paths (list of str): List of paths to the input CSV files.\n",
    "        output_path (str): Path to save the cleaned CSV file.\n",
    "        city_names (list of str): List of city names corresponding to input files.\n",
    "        date_open_col (str): Name of the column containing open date.\n",
    "        date_closed_col (str): Name of the column containing closed date.\n",
    "        department_col (str): Name of the column containing department information.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Merged and cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # Initialize an empty DataFrame to store cleaned data from all parts\n",
    "    cleaned_data = pd.DataFrame()\n",
    "\n",
    "    for input_path, city_name in zip(input_paths, city_names):\n",
    "        # Split the input file if it's larger than 1000 MB\n",
    "        split_files = split_csv(input_path, max_size_mb=1000)\n",
    "\n",
    "        for file in split_files:\n",
    "            try:\n",
    "                # Try loading data with default encoding\n",
    "                try:\n",
    "                    df = pd.read_csv(file)\n",
    "                except UnicodeDecodeError:\n",
    "                    # If default encoding fails, try loading with 'cp1252' encoding\n",
    "                    df = pd.read_csv(file, encoding='cp1252')\n",
    "\n",
    "                # Streamline Data Transformation\n",
    "                df = df[[date_open_col, date_closed_col, department_col]]\n",
    "                df.rename(columns={date_open_col: 'open_date', date_closed_col: 'close_date', department_col: 'department'}, inplace=True)\n",
    "                df['open_date'] = pd.to_datetime(df['open_date'], errors='coerce') # coerce removes unconvertible data\n",
    "                df['close_date'] = pd.to_datetime(df['close_date'], errors='coerce') # coerce removes unconvertible data\n",
    "\n",
    "                # Handling Missing Data (Leaving Missing close dates to account for recent cases that are unclosed)\n",
    "                df.dropna(subset=['open_date','department'], inplace=True)\n",
    "                df['close_date'] = df['close_date'].replace('', pd.NaT)\n",
    "\n",
    "                # Filter data by date range (2014/01/01 -> 2023/12/31)\n",
    "                if \"washington\" or \"miami\" in input_path.lower():\n",
    "                    # Filter data with timezone attached\n",
    "                    df = df[(df['open_date'] >= pd.Timestamp(\"2014-01-01 23:59:59'\").tz_localize(None).tz_localize('UTC')) & (df['open_date'] <= pd.Timestamp(\"2023-12-31 23:59:59'\").tz_localize(None).tz_localize('UTC'))]\n",
    "                    # Drop Timezone (+00:00)\n",
    "                    df['open_date'] = pd.to_datetime(df['open_date']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    df['close_date'] = pd.to_datetime(df['close_date']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                else:\n",
    "                    # Non UTC\n",
    "                    df = df[(df['open_date'] >= pd.Timestamp(\"2014-01-01 23:59:59'\")) & (df['open_date'] <= pd.Timestamp(\"2023-12-31 23:59:59'\"))]\n",
    "\n",
    "                # Add constant column for city name\n",
    "                df['city'] = city_name\n",
    "\n",
    "                # Sort by open_date\n",
    "                df.sort_values(by='open_date', inplace=True)\n",
    "\n",
    "                # Append to cleaned_data DataFrame\n",
    "                cleaned_data = pd.concat([cleaned_data, df])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {file}: {e}\")\n",
    "\n",
    "    # Save the final cleaned data to the specified output file\n",
    "    cleaned_data.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Show dataframe on output\n",
    "    return cleaned_data\n",
    "\n",
    "# These are the function calls run through the clean_city_data function. \n",
    "#clean_city_data([f'../data/raw/boston_{i}.csv' for i in range(2014,2020)],'../data/raw/boston_cleaned_1.csv',        'boston',        'open_dt',                  'closed_dt',               'subject')          \n",
    "#clean_city_data([f'../data/raw/boston_{i}.csv' for i in range(2020,2024)],'../data/raw/boston_cleaned_2.csv',        'boston',        'open_dt',                  'closed_dt',               'subject')\n",
    "#clean_city_data(['../data/raw/buffalo.csv'],       '../data/raw/buffalo_cleaned.csv',      'buffalo',       'Open Date',                'Closed Date',             'Subject') \n",
    "#clean_city_data(['../data/raw/oakland.csv'],      '../data/raw/oakland_cleaned.csv',       'oakland',       'DATETIMEINIT',             'DATETIMECLOSED',          'REQCATEGORY')\n",
    "#clean_city_data(['../data/raw/washington_dc.csv'], '../data/raw/washington_dc_cleaned.csv', 'washington_dc', 'INITIATEDDATE',            'CLOSEDDATE',              'REQUESTCATEGORY')         \n",
    "#clean_city_data([f'../data/new_miami/311_Service_Requests_-_Miami-Dade_County_-_{i}.csv' for i in range(2014,2019)],'../data/raw/miami_cleaned_1.csv','miami','ticket_created_date_time','ticket_closed_date_time','case_owner_description')\n",
    "clean_city_data([f'../data/new_miami/311_Service_Requests_-_Miami-Dade_County_-_{i}.csv' for i in range(2019,2024)],'../data/raw/miami_cleaned_2.csv','miami','ticket_created_date_time','ticket_closed_date_time','case_owner_description')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
